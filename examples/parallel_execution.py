"""Parallel execution example: Zero-latency follow-up generation.

This example shows how to run Compass in parallel with your agent
using LangGraph's Send() API, so follow-ups are ready the moment
your agent finishes.

Run with: python examples/parallel_execution.py
"""

from typing import Annotated

from langchain_core.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import END, START, StateGraph
from langgraph.types import Send
from typing_extensions import TypedDict

from compass import CompassNode


class State(TypedDict):
    """Extended state with query tracking for parallel execution."""

    messages: Annotated[list, lambda x, y: x + y]
    query: str
    compass_suggestions: list[str]


def extract_query(state: State) -> dict:
    """Extract query from messages for parallel processing."""
    messages = state.get("messages", [])
    query = ""
    for msg in messages:
        if isinstance(msg, HumanMessage):
            query = msg.content
            break
    return {"query": query}


def agent_node(state: State) -> dict:
    """Simulated agent that processes the query."""
    query = state.get("query", "")

    # Simulate agent work (in real usage, this calls your LLM/tools)
    response = f"""Based on your question about "{query}", here's what I found:

This is a comprehensive response that provides detailed information about the topic.
The response includes multiple paragraphs to demonstrate how Compass works with
substantial content. In a real application, this would be generated by your LLM
after potentially using tools, RAG, or other agent capabilities."""

    return {"messages": [AIMessage(content=response)]}


def route_parallel(state: State) -> list[Send]:
    """Route to both agent and compass preparation in parallel."""
    return [
        Send("agent", state),
        Send("compass", state),
    ]


def main():
    # Create Compass node
    model = ChatOpenAI(model="gpt-5-nano", temperature=0.7)
    compass = CompassNode(
        model=model,
        strategy="exploratory",
        max_suggestions=3,
        query_key="query",  # Explicit key since we're using custom state
    )

    # Build graph with parallel execution
    builder = StateGraph(State)

    # Add nodes
    builder.add_node("extract", extract_query)
    builder.add_node("agent", agent_node)
    builder.add_node("compass", compass)

    # Sequential: START -> extract
    builder.add_edge(START, "extract")

    # Parallel: extract -> [agent, compass] (both run simultaneously)
    builder.add_conditional_edges("extract", route_parallel)

    # Both converge to END
    builder.add_edge("agent", END)
    builder.add_edge("compass", END)

    graph = builder.compile()

    # Run the graph
    result = graph.invoke(
        {
            "messages": [HumanMessage(content="How do neural networks learn?")],
            "query": "",
            "compass_suggestions": [],
        }
    )

    print("Agent Response:")
    for msg in result["messages"]:
        if isinstance(msg, AIMessage):
            print(msg.content)

    print("\nCompass Suggestions (generated in parallel):")
    for suggestion in result.get("compass_suggestions", []):
        print(f"  - {suggestion}")


if __name__ == "__main__":
    main()
